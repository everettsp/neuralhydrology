{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to Finetune\n",
    "\n",
    "**Before we start**\n",
    "\n",
    "- This tutorial is rendered from a Jupyter notebook that is hosted on GitHub. If you want to run the code yourself, you can find the notebook and configuration files [here](https://github.com/neuralhydrology/neuralhydrology/tree/master/examples/06-Finetuning).\n",
    "- To be able to run this notebook locally, you need to download the publicly available CAMELS US rainfall-runoff dataset and a publicly available extensions for hourly forcing and streamflow data. See the [Data Prerequisites Tutorial](data-prerequisites.nblink) for a detailed description on where to download the data and how to structure your local dataset folder. Note the special [section](data-prerequisites.nblink#CAMELS-US-catchment-attributes) with additional requirements for this tutorial.\n",
    "\n",
    "This tutorial shows how to adapt a pretrained model to a different, eventually much smaller dataset, a concept called finetuning. Finetuning is well-established in machine learning and thus nothing new. Generally speaking, the idea is to use a (very) large and diverse dataset to learn a general understanding of the underlying problem first and then, in a second step, adapt this general model to the target data. Usually, especially if the available target data is limited, pretraining plus finetuning yields (much) better results than only considering the final target data. \n",
    "\n",
    "The connection to hydrology is the following: Often, researchers or operators are only interested in a single basin. However, considering that a Deep Learning (DL) model has to learn all (physical) process understanding from the available training data, it might be understandable that the data records of a single basin might not be enough (see e.g. the presentation linked at [this](https://meetingorganizer.copernicus.org/EGU2020/EGU2020-8855.html) EGU'20 abstract)\n",
    "\n",
    "This is were we apply the concept of pretraining and finetuning: First, we train a DL model (e.g. an LSTM) with a large and diverse, multi-basin dataset (e.g. CAMELS) and then finetune this model to our basin of interest. Everything you need is available in the NeuralHydrology package and in this notebook we will give you an overview of how to actually do it.\n",
    "\n",
    "**Note**: Finetuning can be a tedious task and is usually very sensitive to the learning rate as well as the number of epochs used for finetuning. One reason is that the pretrained models are usually quite large. In fact, most often they are much larger than what would be possible to train for just a single basin. So during finetuning, we have to make sure that this large capacity is not negatively impacting our model results. Common approaches are to a) only allow parts of the model to be adapted during finetuning and/or b) to train with a much lower learning rate. So far, no publication was published that presents a universally working approach for finetuning in hydrology. So be aware that the results may vary and you might need to invest some time before finding a good strategy. However, in our experience it was always possible to get better results _with_ finetuning than without.\n",
    "\n",
    "**To summarize**: If you are interested in getting the best-performing Deep Learning model for a single basin, pretraining on a large and diverse dataset, followed by finetuning the pretrained model on your target basin is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\")\n",
    "from neuralhydrology.nh_run import start_run, eval_run, finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "\n",
    "In the first step, we need to pretrain our model on a large and possibly diverse dataset. Our target basin does not necessarily have to be a part of this dataset, but usually it should be better to include it.\n",
    "\n",
    "For the sake of the demonstration, we will train an LSTM on the CAMELS US dataset and then finetune this model to a random basin. Note that it is possible to use other inputs during pretraining and finetuning, if additional embedding layers (before the LSTM) are used, which we will ignore for now. Furthermore, we will concentrate only on demonstrating the \"how-to\" rather than striving for best-possible performance. To save time and energy, we will only pretrain the model for a small number of epochs. When striving for the best possible performance, you should make sure that you pretrain the model as best as possible, before starting to finetune.\n",
    "\n",
    "We will stick closely to the model and experimental setup from [Kratzert et al. (2019)](https://hess.copernicus.org/articles/23/5089/2019/hess-23-5089-2019.html). To summarize:\n",
    "- A single LSTM layer with a hidden size of 128.\n",
    "- Input sequences are 365 days and the prediction is made at the last timestep.\n",
    "- We will use the same CAMELS attributes, as in the publication mentioned above, as additional inputs at every time step so that the model can learn different hydrological behaviors depending on the catchment properties.\n",
    "\n",
    "For more details, take a look at the config print-out below.\n",
    "\n",
    "**Note**\n",
    "- The config file assumes that the CAMELS US dataset is stored under `data/CAMELS_US` (relative to the main directory of this repository) or a symbolic link exists at this location. Make sure that this folder contains the required subdirectories `basin_mean_forcing`, `usgs_streamflow` and `camels_attributes_v2.0`. If your data is stored at a different location and you can't or don't want to create a symbolic link, you will need to change the `data_dir` argument in the `531_basins.yml` config file that is located in the same directory as this notebook.\n",
    "- By default, the config (`531_basins.yml`) assumes that you have a CUDA-capable NVIDIA GPU (see config argument `device`). In case you don't have any or you have one but want to train on the CPU, you can either change the config argument to `device: cpu` or pass `gpu=-1` to the `start_run()` function. Please note that training such a model on such a large dataset on CPU takes a very long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-25 18:21:11,454: Logging to c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\output.log initialized.\n",
      "2023-01-25 18:21:11,455: ### Folder structure created at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\n",
      "2023-01-25 18:21:11,456: ### Run configurations for cudalstm_531_basins\n",
      "2023-01-25 18:21:11,456: experiment_name: cudalstm_531_basins\n",
      "2023-01-25 18:21:11,456: run_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\n",
      "2023-01-25 18:21:11,457: train_basin_file: 531_basin_list.txt\n",
      "2023-01-25 18:21:11,457: validation_basin_file: 531_basin_list.txt\n",
      "2023-01-25 18:21:11,457: test_basin_file: 531_basin_list.txt\n",
      "2023-01-25 18:21:11,457: train_start_date: 1999-10-01 00:00:00\n",
      "2023-01-25 18:21:11,458: train_end_date: 2008-09-30 00:00:00\n",
      "2023-01-25 18:21:11,458: validation_start_date: 1980-10-01 00:00:00\n",
      "2023-01-25 18:21:11,459: validation_end_date: 1989-09-30 00:00:00\n",
      "2023-01-25 18:21:11,459: test_start_date: 1989-10-01 00:00:00\n",
      "2023-01-25 18:21:11,460: test_end_date: 1999-09-30 00:00:00\n",
      "2023-01-25 18:21:11,460: seed: 123\n",
      "2023-01-25 18:21:11,461: device: cpu\n",
      "2023-01-25 18:21:11,462: validate_every: 1\n",
      "2023-01-25 18:21:11,462: validate_n_random_basins: 531\n",
      "2023-01-25 18:21:11,463: metrics: ['NSE']\n",
      "2023-01-25 18:21:11,463: model: cudalstm\n",
      "2023-01-25 18:21:11,464: head: regression\n",
      "2023-01-25 18:21:11,465: hidden_size: 128\n",
      "2023-01-25 18:21:11,465: initial_forget_bias: 3\n",
      "2023-01-25 18:21:11,466: output_dropout: 0.4\n",
      "2023-01-25 18:21:11,466: output_activation: linear\n",
      "2023-01-25 18:21:11,467: optimizer: Adam\n",
      "2023-01-25 18:21:11,467: loss: NSE\n",
      "2023-01-25 18:21:11,468: learning_rate: {0: 0.001, 1: 0.0005}\n",
      "2023-01-25 18:21:11,468: batch_size: 256\n",
      "2023-01-25 18:21:11,469: epochs: 3\n",
      "2023-01-25 18:21:11,469: clip_gradient_norm: 1\n",
      "2023-01-25 18:21:11,470: predict_last_n: 1\n",
      "2023-01-25 18:21:11,470: seq_length: 365\n",
      "2023-01-25 18:21:11,471: num_workers: 8\n",
      "2023-01-25 18:21:11,471: log_interval: 5\n",
      "2023-01-25 18:21:11,472: log_tensorboard: True\n",
      "2023-01-25 18:21:11,472: save_weights_every: 1\n",
      "2023-01-25 18:21:11,473: save_validation_results: True\n",
      "2023-01-25 18:21:11,473: dataset: camels_us\n",
      "2023-01-25 18:21:11,473: data_dir: D:\\SynologyDrive\\LSH\\CAMELS_US\n",
      "2023-01-25 18:21:11,474: forcings: ['daymet']\n",
      "2023-01-25 18:21:11,474: dynamic_inputs: ['prcp(mm/day)', 'srad(W/m2)', 'tmax(C)', 'tmin(C)', 'vp(Pa)']\n",
      "2023-01-25 18:21:11,475: target_variables: ['QObs(mm/d)']\n",
      "2023-01-25 18:21:11,475: static_attributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']\n",
      "2023-01-25 18:21:11,476: number_of_basins: 531\n",
      "2023-01-25 18:21:11,476: train_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\train_data\n",
      "2023-01-25 18:21:11,477: img_log_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\img_log\n",
      "2023-01-25 18:21:11,482: ### Device cpu will be used for training\n",
      "2023-01-25 18:21:11,675: Loading basin data into xarray data set.\n",
      "100%|██████████| 531/531 [00:55<00:00,  9.65it/s]\n",
      "2023-01-25 18:22:06,991: Calculating target variable stds per basin\n",
      " 59%|█████▉    | 314/531 [00:00<00:00, 1573.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\basedataset.py:460: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  per_basin_target_stds = torch.tensor([np.nanstd(obs, axis=1)], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:00<00:00, 1574.23it/s]\n",
      "2023-01-25 18:22:07,422: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 531/531 [00:11<00:00, 45.98it/s]\n",
      "2023-01-25 18:22:19,264: Setting learning rate to 0.0005\n",
      "# Epoch 1: 100%|██████████| 6821/6821 [2:43:35<00:00,  1.44s/it, Loss: 0.0020]  \n",
      "2023-01-25 21:05:54,603: Epoch 1 average loss: 0.03716465793238026\n",
      "# Validation:  29%|██▉       | 156/531 [20:45<50:33,  8.09s/it] 2023-01-25 21:26:40,522: The following basins had not enough valid target values to calculate a standard deviation: 02427250. NSE loss values for this basin will be NaN.\n",
      "# Validation:  79%|███████▊  | 418/531 [55:37<15:01,  7.98s/it]2023-01-25 22:01:31,963: The following basins had not enough valid target values to calculate a standard deviation: 09484600. NSE loss values for this basin will be NaN.\n",
      "# Validation: 100%|██████████| 531/531 [1:10:37<00:00,  7.98s/it]\n",
      "2023-01-25 22:16:32,016: Stored results at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\validation\\model_epoch001\\validation_results.p\n",
      "2023-01-25 22:16:32,031: Epoch 1 average validation loss: 0.02995 -- Median validation metrics: NSE: 0.63487\n",
      "# Epoch 2: 100%|██████████| 6821/6821 [2:45:01<00:00,  1.45s/it, Loss: 0.0276]  \n",
      "2023-01-26 01:01:33,953: Epoch 2 average loss: 0.023511703823668656\n",
      "# Validation: 100%|██████████| 531/531 [1:10:12<00:00,  7.93s/it]\n",
      "2023-01-26 02:11:47,083: Stored results at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\validation\\model_epoch002\\validation_results.p\n",
      "2023-01-26 02:11:47,093: Epoch 2 average validation loss: 0.03437 -- Median validation metrics: NSE: 0.59430\n",
      "# Epoch 3: 100%|██████████| 6821/6821 [2:44:39<00:00,  1.45s/it, Loss: 0.0320]  \n",
      "2023-01-26 04:56:27,007: Epoch 3 average loss: 0.021101977223633633\n",
      "# Validation: 100%|██████████| 531/531 [1:10:45<00:00,  8.00s/it]\n",
      "2023-01-26 06:07:12,584: Stored results at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\\validation\\model_epoch003\\validation_results.p\n",
      "2023-01-26 06:07:12,598: Epoch 3 average validation loss: 0.02891 -- Median validation metrics: NSE: 0.68771\n"
     ]
    }
   ],
   "source": [
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"531_basins.yml\"))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(\"531_basins.yml\"), gpu=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end with an okay'ish model that should be enough for the purpose of this demonstration. Remember we only train for a limited number of epochs here.\n",
    "\n",
    "Next, we'll load the validation results into memory so we can select a basin to demonstrate how to finetune based on the model performance. \n",
    "Since the folder name is created dynamically (including the date and time of the start of the run) you will need to change the `run_dir` argument according to your local directory name. \n",
    "\n",
    "Here, we will select a random basin from the lower 50% of the NSE distribution, i.e. a basin where the NSE is below the median NSE. Usually, you'll see better performance gains for basins with lower model performance than for those where the base model is already really good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median NSE of the validation period 0.688\n",
      "Selected basin: 02069700 with an NSE of 0.621\n"
     ]
    }
   ],
   "source": [
    "# Load validation results from the last epoch\n",
    "run_dir = Path(\"runs/cudalstm_531_basins_2501_182111/\")\n",
    "df = pd.read_csv(run_dir / \"validation\" / \"model_epoch003\" / \"validation_metrics.csv\", dtype={'basin': str})\n",
    "df = df.set_index('basin')\n",
    "\n",
    "# Compute the median NSE from all basins, where discharge observations are available for that period\n",
    "print(f\"Median NSE of the validation period {df['NSE'].median():.3f}\")\n",
    "\n",
    "# Select a random basins from the lower 50% of the NSE distribution\n",
    "basin = df.loc[df[\"NSE\"] < df[\"NSE\"].median()].sample(n=1).index[0]\n",
    "\n",
    "print(f\"Selected basin: {basin} with an NSE of {df.loc[df.index == basin, 'NSE'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Next, we will show how to perform finetuning for the basin selected above, based on the model we just trained. The function to use is `finetune` from `neuralhydrology.nh_run` if you want to train from within a script or notebook. If you want to start finetuning from the command line, you can also call the `nh-run` utility with the `finetune` argument, instead of e.g. `train` or `evaluate`.\n",
    "\n",
    "The only thing required, similar to the model training itself, is a config file. This config however has slightly different requirements to a normal model config and works slightly different:\n",
    "- The config has to contain the following two arguments:\n",
    "    - `base_run_dir`: The path to the directory of the pre-trained model.\n",
    "    - `finetune_modules`: Which parts of the pre-trained model you want to finetune. Check the documentation of each model class for a list of all possible parts. Often only parts, e.g. the output layer, are trained during finetuning and the rest is kept fixed. There is no general rule of thumb and most likely you will have to try both.\n",
    "- Any additional argument contained in this config will overwrite the config argument of the pre-trained model. Everything _not_ specified will be taken from the pre-trained model. That is, you can e.g. specify a new basin file in the finetuning config (by `train_basin_file`) to finetune the pre-trained model on a different set of basins, or even just a single basin as we will do in this notebook. You can also change the learning rate, loss function, evaluation metrics and so on. The only thing you can not change are arguments that change the model architecture (e.g. `model`, `hidden_size` etc.), because this leads to errors when you try to load the pre-trained weights into the initialized model.\n",
    "\n",
    "Let's have a look at the `finetune.yml` config that we prepared for this tutorial (you can find the file in the same directory as this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat finetune.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of the two arguments that are required, `base_run_dir` is still missing. We will add the argument from here and point at the directory of the model we just trained. Furthermore, we point to a new file for training, validation and testing, called `finetune_basin.txt`, which does not yet exist. We will create this file and add the basin we selected above as the only basin we want to use here. The rest are some changes to the learning rate and the number of training epochs as well as a new name. Also note that here, we train the full model, by selecting all model parts available for the `CudaLSTM` under `finetune_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the pre-trained model to the finetune config\n",
    "with open(\"finetune.yml\", \"a\") as fp:\n",
    "    fp.write(f\"\\nbase_run_dir: {run_dir.absolute()}\")\n",
    "    \n",
    "# Create a basin file with the basin we selected above\n",
    "with open(\"finetune_basin.txt\", \"w\") as fp:\n",
    "    fp.write(basin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we are ready to start the finetuning. As mentioned above, we have two options to start finetuning:\n",
    "1. Call the `finetune()` function from a different Python script or a Jupyter Notebook with the path to the config.\n",
    "2. Start the finetuning from the command line by calling\n",
    "\n",
    "```bash\n",
    "nh-run finetune --config-file /path/to/config.yml\n",
    "```\n",
    "\n",
    "Here, we will use the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-26 10:45:08,278: Logging to c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_finetuned_2601_104508\\output.log initialized.\n",
      "2023-01-26 10:45:08,279: ### Folder structure created at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_finetuned_2601_104508\n",
      "2023-01-26 10:45:08,280: ### Start finetuning with pretrained model stored in c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\n",
      "2023-01-26 10:45:08,280: ### Run configurations for cudalstm_531_basins_finetuned\n",
      "2023-01-26 10:45:08,281: batch_size: 256\n",
      "2023-01-26 10:45:08,282: clip_gradient_norm: 1\n",
      "2023-01-26 10:45:08,282: commit_hash: None\n",
      "2023-01-26 10:45:08,283: data_dir: D:\\SynologyDrive\\LSH\\CAMELS_US\n",
      "2023-01-26 10:45:08,284: dataset: camels_us\n",
      "2023-01-26 10:45:08,284: device: cpu\n",
      "2023-01-26 10:45:08,285: dynamic_inputs: ['prcp(mm/day)', 'srad(W/m2)', 'tmax(C)', 'tmin(C)', 'vp(Pa)']\n",
      "2023-01-26 10:45:08,285: epochs: 10\n",
      "2023-01-26 10:45:08,286: experiment_name: cudalstm_531_basins_finetuned\n",
      "2023-01-26 10:45:08,286: forcings: ['daymet']\n",
      "2023-01-26 10:45:08,287: head: regression\n",
      "2023-01-26 10:45:08,287: hidden_size: 128\n",
      "2023-01-26 10:45:08,288: img_log_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_finetuned_2601_104508\\img_log\n",
      "2023-01-26 10:45:08,288: initial_forget_bias: 3\n",
      "2023-01-26 10:45:08,289: learning_rate: {0: 0.0005, 2: 5e-05}\n",
      "2023-01-26 10:45:08,289: log_interval: 5\n",
      "2023-01-26 10:45:08,290: log_tensorboard: True\n",
      "2023-01-26 10:45:08,291: loss: NSE\n",
      "2023-01-26 10:45:08,291: metrics: ['NSE']\n",
      "2023-01-26 10:45:08,292: model: cudalstm\n",
      "2023-01-26 10:45:08,292: num_workers: 8\n",
      "2023-01-26 10:45:08,293: number_of_basins: 1\n",
      "2023-01-26 10:45:08,294: optimizer: Adam\n",
      "2023-01-26 10:45:08,294: output_activation: linear\n",
      "2023-01-26 10:45:08,295: output_dropout: 0.4\n",
      "2023-01-26 10:45:08,295: package_version: 1.5.0\n",
      "2023-01-26 10:45:08,296: predict_last_n: 1\n",
      "2023-01-26 10:45:08,297: run_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_finetuned_2601_104508\n",
      "2023-01-26 10:45:08,297: save_validation_results: True\n",
      "2023-01-26 10:45:08,298: save_weights_every: 1\n",
      "2023-01-26 10:45:08,299: seed: 123\n",
      "2023-01-26 10:45:08,299: seq_length: 365\n",
      "2023-01-26 10:45:08,300: static_attributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']\n",
      "2023-01-26 10:45:08,300: target_variables: ['QObs(mm/d)']\n",
      "2023-01-26 10:45:08,301: test_basin_file: finetune_basin.txt\n",
      "2023-01-26 10:45:08,302: test_end_date: 1999-09-30 00:00:00\n",
      "2023-01-26 10:45:08,302: test_start_date: 1989-10-01 00:00:00\n",
      "2023-01-26 10:45:08,302: train_basin_file: finetune_basin.txt\n",
      "2023-01-26 10:45:08,303: train_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_finetuned_2601_104508\\train_data\n",
      "2023-01-26 10:45:08,303: train_end_date: 2008-09-30 00:00:00\n",
      "2023-01-26 10:45:08,304: train_start_date: 1999-10-01 00:00:00\n",
      "2023-01-26 10:45:08,305: validate_every: 1\n",
      "2023-01-26 10:45:08,305: validate_n_random_basins: 531\n",
      "2023-01-26 10:45:08,305: validation_basin_file: finetune_basin.txt\n",
      "2023-01-26 10:45:08,306: validation_end_date: 1989-09-30 00:00:00\n",
      "2023-01-26 10:45:08,307: validation_start_date: 1980-10-01 00:00:00\n",
      "2023-01-26 10:45:08,307: finetune_modules: ['head', 'lstm']\n",
      "2023-01-26 10:45:08,307: base_run_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\06-Finetuning\\runs\\cudalstm_531_basins_2501_182111\n",
      "2023-01-26 10:45:08,308: is_finetuning: True\n",
      "2023-01-26 10:45:08,308: is_continue_training: False\n",
      "2023-01-26 10:45:08,314: ### Device cpu will be used for training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following attributes have a std of zero or NaN, which results in NaN's when normalizing the features. Remove the attributes from the attribute feature list and restart the run. \nAttributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18296\\3717755737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinetune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"finetune.yml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\nh_run.py\u001b[0m in \u001b[0;36mfinetune\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\training\\train.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Unknown head {cfg.head}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_and_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py\u001b[0m in \u001b[0;36minitialize_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcontinue_training\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mwill\u001b[0m \u001b[0malso\u001b[0m \u001b[0mrestore\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset contains no samples.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\training\\basetrainer.py\u001b[0m in \u001b[0;36m_get_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mBaseDataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\__init__.py\u001b[0m in \u001b[0;36mget_dataset\u001b[1;34m(cfg, is_train, period, basin, additional_features, id_to_int, scaler)\u001b[0m\n\u001b[0;32m     87\u001b[0m                  \u001b[0madditional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madditional_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                  \u001b[0mid_to_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                  scaler=scaler)\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\camelsus.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cfg, is_train, period, basin, additional_features, id_to_int, scaler)\u001b[0m\n\u001b[0;32m     64\u001b[0m                                        \u001b[0madditional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madditional_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                                        \u001b[0mid_to_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                                        scaler=scaler)\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_load_basin_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasin\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\basedataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cfg, is_train, period, basin, additional_features, id_to_int, scaler)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# load and preprocess data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\basedataset.py\u001b[0m in \u001b[0;36m_load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# load attributes first to sanity-check those features before doing the compute expensive time series loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_combined_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[0mxr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_or_create_xarray_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datasetzoo\\basedataset.py\u001b[0m in \u001b[0;36m_load_combined_attributes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;31m# in case of training (not finetuning) check for NaNs in feature std.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_scaler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                 \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributes_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\neuralhydrology\\neuralhydrology\\datautils\\utils.py\u001b[0m in \u001b[0;36mattributes_sanity_check\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;34m\"and restart the run. \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"Attributes: {attributes}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         ]\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# Check for NaNs in any attribute of any basin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following attributes have a std of zero or NaN, which results in NaN's when normalizing the features. Remove the attributes from the attribute feature list and restart the run. \nAttributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']"
     ]
    }
   ],
   "source": [
    "finetune(Path(\"finetune.yml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the validation result, we can see an increase of roughly 0.05 NSE.\n",
    "\n",
    "Last but not least, we will compare the pre-trained and the finetuned model on the test period. For this, we will make use of the `eval_run` function from `neuralhydrolgy.nh_run`. Alternatively, you could evaluate both runs from the command line by calling\n",
    "\n",
    "```bash\n",
    "nh-run evaluate --run-dir /path/to/run_directory/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-09 14:16:09,586: Using the model weights from runs/cudalstm_531_basins_0901_135400/model_epoch003.pt\n",
      "# Evaluation: 100%|██████████| 531/531 [02:09<00:00,  4.11it/s]\n",
      "2022-01-09 14:18:18,959: Stored results at runs/cudalstm_531_basins_0901_135400/test/model_epoch003/test_results.p\n"
     ]
    }
   ],
   "source": [
    "eval_run(run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `eval_run()` function as above, but pointing to the directory of the finetuned run. By default, this function evaluates the last checkpoint, which can be changed with the `epoch` argument. Here however, we use the default. Again, if you want to run this notebook locally, make sure to adapt the folder name of the finetune run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-09 14:19:06,488: Using the model weights from runs/cudalstm_531_basins_finetuned_0901_141548/model_epoch010.pt\n",
      "# Evaluation: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "2022-01-09 14:19:06,726: Stored results at runs/cudalstm_531_basins_finetuned_0901_141548/test/model_epoch010/test_results.p\n"
     ]
    }
   ],
   "source": [
    "finetune_dir = Path(\"runs/cudalstm_531_basins_finetuned_0901_141548\")\n",
    "eval_run(finetune_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the test period results of the pre-trained base model and the finetuned model for the basin that we chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basin 02112360 base model performance: 0.303\n",
      "Performance after finetuning: 0.580\n"
     ]
    }
   ],
   "source": [
    "# load test results of the base run\n",
    "df_pretrained = pd.read_csv(run_dir / \"test/model_epoch003/test_metrics.csv\", dtype={'basin': str})\n",
    "df_pretrained = df_pretrained.set_index(\"basin\")\n",
    "    \n",
    "# load test results of the finetuned model\n",
    "df_finetuned = pd.read_csv(finetune_dir / \"test/model_epoch010/test_metrics.csv\", dtype={'basin': str})\n",
    "df_finetuned = df_finetuned.set_index(\"basin\")\n",
    "    \n",
    "# extract basin performance\n",
    "base_model_nse = df_pretrained.loc[df_pretrained.index == basin, \"NSE\"].values[0]\n",
    "finetune_nse = df_finetuned.loc[df_finetuned.index == basin, \"NSE\"].values[0]\n",
    "print(f\"Basin {basin} base model performance: {base_model_nse:.3f}\")\n",
    "print(f\"Performance after finetuning: {finetune_nse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see roughly the same performance increase in the test period (slightly higher), which is great. However, note that a) our base model was not optimally trained (we stopped quite early) but also b) the finetuning settings were chosen rather randomly. From our experience so far, you can almost always get performance increases for individual basins with finetuning, but it is difficult to find settings that are universally applicable. However, this tutorial was just a showcase of how easy it actually is to finetune models with the NeuralHydrology library. Now it is up to you to experiment with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('neuralhydrology')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aa8ee6c85fe5f665667a040fb2448b4f04355b5dff35716c2ee86b5644c8b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
