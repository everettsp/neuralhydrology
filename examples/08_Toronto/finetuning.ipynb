{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-to Finetune\n",
    "\n",
    "**Before we start**\n",
    "\n",
    "- This tutorial is rendered from a Jupyter notebook that is hosted on GitHub. If you want to run the code yourself, you can find the notebook and configuration files [here](https://github.com/neuralhydrology/neuralhydrology/tree/master/examples/06-Finetuning).\n",
    "- To be able to run this notebook locally, you need to download the publicly available CAMELS US rainfall-runoff dataset and a publicly available extensions for hourly forcing and streamflow data. See the [Data Prerequisites Tutorial](data-prerequisites.nblink) for a detailed description on where to download the data and how to structure your local dataset folder. Note the special [section](data-prerequisites.nblink#CAMELS-US-catchment-attributes) with additional requirements for this tutorial.\n",
    "\n",
    "This tutorial shows how to adapt a pretrained model to a different, eventually much smaller dataset, a concept called finetuning. Finetuning is well-established in machine learning and thus nothing new. Generally speaking, the idea is to use a (very) large and diverse dataset to learn a general understanding of the underlying problem first and then, in a second step, adapt this general model to the target data. Usually, especially if the available target data is limited, pretraining plus finetuning yields (much) better results than only considering the final target data. \n",
    "\n",
    "The connection to hydrology is the following: Often, researchers or operators are only interested in a single basin. However, considering that a Deep Learning (DL) model has to learn all (physical) process understanding from the available training data, it might be understandable that the data records of a single basin might not be enough (see e.g. the presentation linked at [this](https://meetingorganizer.copernicus.org/EGU2020/EGU2020-8855.html) EGU'20 abstract)\n",
    "\n",
    "This is were we apply the concept of pretraining and finetuning: First, we train a DL model (e.g. an LSTM) with a large and diverse, multi-basin dataset (e.g. CAMELS) and then finetune this model to our basin of interest. Everything you need is available in the NeuralHydrology package and in this notebook we will give you an overview of how to actually do it.\n",
    "\n",
    "**Note**: Finetuning can be a tedious task and is usually very sensitive to the learning rate as well as the number of epochs used for finetuning. One reason is that the pretrained models are usually quite large. In fact, most often they are much larger than what would be possible to train for just a single basin. So during finetuning, we have to make sure that this large capacity is not negatively impacting our model results. Common approaches are to a) only allow parts of the model to be adapted during finetuning and/or b) to train with a much lower learning rate. So far, no publication was published that presents a universally working approach for finetuning in hydrology. So be aware that the results may vary and you might need to invest some time before finding a good strategy. However, in our experience it was always possible to get better results _with_ finetuning than without.\n",
    "\n",
    "**To summarize**: If you are interested in getting the best-performing Deep Learning model for a single basin, pretraining on a large and diverse dataset, followed by finetuning the pretrained model on your target basin is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from neuralhydrology.nh_run import start_run, eval_run, finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end with an okay'ish model that should be enough for the purpose of this demonstration. Remember we only train for a limited number of epochs here.\n",
    "\n",
    "Next, we'll load the validation results into memory so we can select a basin to demonstrate how to finetune based on the model performance. \n",
    "Since the folder name is created dynamically (including the date and time of the start of the run) you will need to change the `run_dir` argument according to your local directory name. \n",
    "\n",
    "Here, we will select a random basin from the lower 50% of the NSE distribution, i.e. a basin where the NSE is below the median NSE. Usually, you'll see better performance gains for basins with lower model performance than for those where the base model is already really good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation results from the last epoch\n",
    "run_dir = Path(\"runs/test_mts_camels_augmented_64_2606_092458/\")\n",
    "\n",
    "# Add the path to the pre-trained model to the finetune config\n",
    "#with open(\"finetune.yml\", \"a\") as fp:\n",
    "#    fp.write(f\"\\nbase_run_dir: {run_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we are ready to start the finetuning. As mentioned above, we have two options to start finetuning:\n",
    "1. Call the `finetune()` function from a different Python script or a Jupyter Notebook with the path to the config.\n",
    "2. Start the finetuning from the command line by calling\n",
    "\n",
    "```bash\n",
    "nh-run finetune --config-file /path/to/config.yml\n",
    "```\n",
    "\n",
    "Here, we will use the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-30 14:09:04,008: Logging to c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\mts_finetuned_3007_140904\\output.log initialized.\n",
      "2025-07-30 14:09:04,009: ### Folder structure created at c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\mts_finetuned_3007_140904\n",
      "2025-07-30 14:09:04,009: ### Start finetuning with pretrained model stored in c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\test_mts_camels_augmented_64_2606_092458\n",
      "2025-07-30 14:09:04,011: ### Run configurations for mts_finetuned\n",
      "2025-07-30 14:09:04,011: allow_subsequent_nan_losses: 8\n",
      "2025-07-30 14:09:04,012: batch_size: 256\n",
      "2025-07-30 14:09:04,013: clip_gradient_norm: 1\n",
      "2025-07-30 14:09:04,014: clip_targets_to_zero: ['qobs_mm_per_hour']\n",
      "2025-07-30 14:09:04,015: commit_hash: d99595d\n",
      "2025-07-30 14:09:04,015: data_dir: F:\\Data\\LSH\\CAMELS_US_TORONTO\n",
      "2025-07-30 14:09:04,016: dataset: hourly_camels_usto\n",
      "2025-07-30 14:09:04,017: device: cuda:0\n",
      "2025-07-30 14:09:04,018: dynamic_inputs: {'1D': ['total_precipitation', 'temperature', 'pressure'], '1h': ['total_precipitation', 'temperature', 'pressure']}\n",
      "2025-07-30 14:09:04,019: epochs: 32\n",
      "2025-07-30 14:09:04,020: experiment_name: mts_finetuned\n",
      "2025-07-30 14:09:04,020: forcings: ['nldas_hourly']\n",
      "2025-07-30 14:09:04,022: head: regression\n",
      "2025-07-30 14:09:04,022: hidden_size: 64\n",
      "2025-07-30 14:09:04,024: img_log_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\mts_finetuned_3007_140904\\img_log\n",
      "2025-07-30 14:09:04,025: initial_forget_bias: 3\n",
      "2025-07-30 14:09:04,026: learning_rate: {0: 0.0005, 2: 5e-05}\n",
      "2025-07-30 14:09:04,027: log_interval: 5\n",
      "2025-07-30 14:09:04,028: log_n_figures: 8\n",
      "2025-07-30 14:09:04,029: log_tensorboard: True\n",
      "2025-07-30 14:09:04,029: loss: NSE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-30 14:09:04,031: metrics: ['NSE', 'KGE', 'PI']\n",
      "2025-07-30 14:09:04,032: model: mtslstm\n",
      "2025-07-30 14:09:04,033: num_workers: 12\n",
      "2025-07-30 14:09:04,035: number_of_basins: 1\n",
      "2025-07-30 14:09:04,036: optimizer: Adam\n",
      "2025-07-30 14:09:04,037: output_activation: linear\n",
      "2025-07-30 14:09:04,039: output_dropout: 0.4\n",
      "2025-07-30 14:09:04,040: package_version: 1.12.0\n",
      "2025-07-30 14:09:04,041: predict_last_n: {'1D': 1, '1h': 24}\n",
      "2025-07-30 14:09:04,043: regularization: ['tie_frequencies']\n",
      "2025-07-30 14:09:04,044: run_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\mts_finetuned_3007_140904\n",
      "2025-07-30 14:09:04,045: save_weights_every: 1\n",
      "2025-07-30 14:09:04,046: seed: 721845\n",
      "2025-07-30 14:09:04,046: seq_length: {'1D': 365, '1h': 120}\n",
      "2025-07-30 14:09:04,047: shared_mtslstm: False\n",
      "2025-07-30 14:09:04,048: target_variables: ['qobs_mm_per_hour']\n",
      "2025-07-30 14:09:04,048: test_basin_file: camus_basins.txt\n",
      "2025-07-30 14:09:04,049: test_end_date: 2024-01-01 00:00:00\n",
      "2025-07-30 14:09:04,050: test_start_date: 2020-01-01 00:00:00\n",
      "2025-07-30 14:09:04,050: train_basin_file: camus_basins.txt\n",
      "2025-07-30 14:09:04,052: train_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\mts_finetuned_3007_140904\\train_data\n",
      "2025-07-30 14:09:04,053: train_end_date: 2016-01-01 00:00:00\n",
      "2025-07-30 14:09:04,053: train_start_date: 2008-01-01 00:00:00\n",
      "2025-07-30 14:09:04,054: transfer_mtslstm_states: {'h': 'linear', 'c': 'linear'}\n",
      "2025-07-30 14:09:04,054: use_frequencies: ['1h', '1D']\n",
      "2025-07-30 14:09:04,055: validate_every: 8\n",
      "2025-07-30 14:09:04,056: validate_n_random_basins: 32\n",
      "2025-07-30 14:09:04,056: validation_basin_file: camus_basins.txt\n",
      "2025-07-30 14:09:04,057: validation_end_date: 2020-01-01 00:00:00\n",
      "2025-07-30 14:09:04,057: validation_start_date: 2016-01-01 00:00:00\n",
      "2025-07-30 14:09:04,058: finetune_modules: ['head']\n",
      "2025-07-30 14:09:04,058: base_run_dir: c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\test_mts_camels_augmented_64_2606_092458\n",
      "2025-07-30 14:09:04,059: is_finetuning: True\n",
      "2025-07-30 14:09:04,059: is_continue_training: False\n",
      "2025-07-30 14:09:04,167: ### Device cuda:0 will be used for training\n",
      "2025-07-30 14:09:04,194: Loading basin data into xarray data set.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "2025-07-30 14:09:05,147: Calculating target variable stds per basin\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.65it/s]\n",
      "2025-07-30 14:09:05,194: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.28s/it]\n",
      "2025-07-30 14:09:09,577: No specific hidden size for frequencies are specified. Same hidden size is used for all.\n",
      "2025-07-30 14:09:12,001: Starting training from checkpoint c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\runs\\test_mts_camels_augmented_64_2606_092458\\model_epoch128.pt\n",
      "2025-07-30 14:09:12,137: Could not resolve the following module parts for finetuning: ['head']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\../..\\neuralhydrology\\training\\basetrainer.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(str(checkpoint_path), map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Epoch 1:   0%|          | 0/12 [00:46<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinetune.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\../..\\neuralhydrology\\nh_run.py:147\u001b[0m, in \u001b[0;36mfinetune\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gpu \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    145\u001b[0m     config\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\../..\\neuralhydrology\\training\\train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize_training()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\../..\\neuralhydrology\\training\\basetrainer.py:214\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    212\u001b[0m         param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlearning_rate[epoch]\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m avg_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_logger\u001b[38;5;241m.\u001b[39msummarise()\n\u001b[0;32m    216\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32mc:\\Users\\everett\\Documents\\GitHub\\neuralhydrology\\examples\\08_Toronto\\../..\\neuralhydrology\\training\\basetrainer.py:321\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m# get gradients\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mclip_gradient_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mclip_gradient_norm)\n",
      "File \u001b[1;32mc:\\Users\\everett\\.conda\\envs\\neuralhydrology\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\everett\\.conda\\envs\\neuralhydrology\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\everett\\.conda\\envs\\neuralhydrology\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "finetune(Path(\"finetune.yml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the validation result, we can see an increase of roughly 0.05 NSE.\n",
    "\n",
    "Last but not least, we will compare the pre-trained and the finetuned model on the test period. For this, we will make use of the `eval_run` function from `neuralhydrolgy.nh_run`. Alternatively, you could evaluate both runs from the command line by calling\n",
    "\n",
    "```bash\n",
    "nh-run evaluate --run-dir /path/to/run_directory/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-09 14:16:09,586: Using the model weights from runs/cudalstm_531_basins_0901_135400/model_epoch003.pt\n",
      "# Evaluation: 100%|██████████| 531/531 [02:09<00:00,  4.11it/s]\n",
      "2022-01-09 14:18:18,959: Stored results at runs/cudalstm_531_basins_0901_135400/test/model_epoch003/test_results.p\n"
     ]
    }
   ],
   "source": [
    "eval_run(run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the `eval_run()` function as above, but pointing to the directory of the finetuned run. By default, this function evaluates the last checkpoint, which can be changed with the `epoch` argument. Here however, we use the default. Again, if you want to run this notebook locally, make sure to adapt the folder name of the finetune run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-09 14:19:06,488: Using the model weights from runs/cudalstm_531_basins_finetuned_0901_141548/model_epoch010.pt\n",
      "# Evaluation: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "2022-01-09 14:19:06,726: Stored results at runs/cudalstm_531_basins_finetuned_0901_141548/test/model_epoch010/test_results.p\n"
     ]
    }
   ],
   "source": [
    "finetune_dir = Path(\"runs/cudalstm_531_basins_finetuned_0901_141548\")\n",
    "eval_run(finetune_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the test period results of the pre-trained base model and the finetuned model for the basin that we chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basin 02112360 base model performance: 0.303\n",
      "Performance after finetuning: 0.580\n"
     ]
    }
   ],
   "source": [
    "# load test results of the base run\n",
    "df_pretrained = pd.read_csv(run_dir / \"test/model_epoch003/test_metrics.csv\", dtype={'basin': str})\n",
    "df_pretrained = df_pretrained.set_index(\"basin\")\n",
    "    \n",
    "# load test results of the finetuned model\n",
    "df_finetuned = pd.read_csv(finetune_dir / \"test/model_epoch010/test_metrics.csv\", dtype={'basin': str})\n",
    "df_finetuned = df_finetuned.set_index(\"basin\")\n",
    "    \n",
    "# extract basin performance\n",
    "base_model_nse = df_pretrained.loc[df_pretrained.index == basin, \"NSE\"].values[0]\n",
    "finetune_nse = df_finetuned.loc[df_finetuned.index == basin, \"NSE\"].values[0]\n",
    "print(f\"Basin {basin} base model performance: {base_model_nse:.3f}\")\n",
    "print(f\"Performance after finetuning: {finetune_nse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see roughly the same performance increase in the test period (slightly higher), which is great. However, note that a) our base model was not optimally trained (we stopped quite early) but also b) the finetuning settings were chosen rather randomly. From our experience so far, you can almost always get performance increases for individual basins with finetuning, but it is difficult to find settings that are universally applicable. However, this tutorial was just a showcase of how easy it actually is to finetune models with the NeuralHydrology library. Now it is up to you to experiment with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
