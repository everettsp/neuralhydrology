# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: cudalstm_531_basins

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: toronto_plus_clustered(n=128, k=4).txt
validation_basin_file: toronto_plus_clustered(n=128, k=4).txt
test_basin_file: toronto_plus_clustered(n=128, k=4).txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: "01/01/2008"
train_end_date: "01/01/2016"
validation_start_date: "01/01/2016"
validation_end_date: "01/01/2020"
test_start_date: "01/01/2020"
test_end_date: "01/01/2024"

# fixed seed, leave empty to use a random seed
seed: 0

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu, mps or None]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 10

# specify how many random basins to use for validation
validate_n_random_basins: 16

# specify which metrics to calculate during validation (see codebase.evaluation.metrics)
metrics:
- NSE
- KGE
- PI

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, shortcutlstm, dropoutlstm, cudalstminitialh]
# (has to match the if statement in modelzoo/__init__.py)
model: cudalstm

# prediction head [regression, mdn, umal]. Define the head specific parameters below
head: regression

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

output_activation: linear

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam, Adadelta]
optimizer: Adam

# specify loss [MSE, NSE, RMSE, UMALLoss, MDNLoss]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 0.1
  20: 0.01
  40: 0.001
  60: 0.0001
  80: 1e-05

# Mini-batch size
batch_size: 256

# Number of training epochs
epochs: 128

# If True, clips norm of gradients
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length
predict_last_n: 1

# Length of the input sequence
seq_length: 12

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: True

# Save model weights every n epochs
save_weights_every: 1

# Store the results of the validation to disk
save_validation_results: True

# --- Data configurations --------------------------------------------------------------------------

dataset: hourly_camels_usto

# Path to CAMELS data set
data_dir: F:\Data\LSH\CAMELS_US_TORONTO

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended]
# can be either a list of forcings or a single forcing product
forcings:
- nldas_hourly

# variables to use as time series input (names match the data file column headers)
# Note: In case of multiple input forcing products, you have to append the forcing product behind
# each variable. E.g. 'prcp(mm/day)' of the daymet product is 'prcp(mm/day)_daymet'
dynamic_inputs:
- total_precipitation
- temperature
- pressure

# which columns to use as target
target_variables:
- qobs_mm_per_hour

static_attributes:
- pet_mean
- p_seasonality
- q_mean
- gauge_lat
- high_q_dur
- p_mean
- elev_mean
- high_q_freq
- runoff_ratio
- q5
- low_prec_dur
- frac_forest
- high_prec_dur
- frac_snow
- low_q_dur
- gauge_lon
- zero_q_freq
- q95
- aridity
- area_gages2
- low_prec_freq
- high_prec_freq
- slope_mean